---
title: "Human Activity Recognition"
author: "Eddy Delta"
date: "05/02/2019"
output: 
  md_document:
    variant: markdown_github
  html_document: default
  pdf_document: default
subtitle: Weight Lifting Exercises Dataset
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
## Context
Research on activity recognition has traditionally focused on discriminating between different activities, i.e. to predict *“which”* activity was performed at a specific point in time. The quality of executing an activity, the *“how (well)”*, has only received little attention so far, even though it potentially provides useful information for a large variety of applications. In this work we define quality of execution and investigate three aspects that pertain to qualitative activity recognition: specifying correct execution, detecting execution mistakes, providing feedback on the to the user.

In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset). 

The data for this project come from this study: [http://groupware.les.inf.puc-rio.br/har](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

The following links provides the [informations about the authors](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) and [Documentation](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) about the original study.

## Data
The training data for this project are available here:

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available here:

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (**Class A**), throwing the elbows to the front (**Class B**), lifting the dumbbell only halfway (**Class C**), lowering the dumbbell only halfway (**Class D**) and throwing the hips to the front (**Class E**). **Class A** corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes, [Read more](http://groupware.les.inf.puc-rio.br/har#ixzz5f70D9k00).

# Exploratory data analysis
```{r original_data, include=FALSE}
# setwd('~/projects/training/datascience/machine_learning/wle')
# https://htmlpreview.github.io/?https://github.com/ome9ax/datascience/blob/master/machine_learning/wle/wle.html
required.packages <- c('doParallel', 'caret', 'ggplot2', 'randomForest', 'rpart', 'rattle') # 'beepr', 'pgmm'
missing.packages <- setdiff(required.packages, rownames(installed.packages()))
if (length(missing.packages) > 0) install.packages(missing.packages)
sapply(required.packages, require, character.only = TRUE)

# library(doParallel)
registerDoParallel(cores = detectCores() - 1)
getDoParWorkers()
# Quick look at the original dataset
# x <- read.csv2(unz(file.path('machine_learning', 'har', 'data', 'dataset-har-PUC-Rio-ugulino.zip'), file.path('dataset-har-PUC-Rio-ugulino.csv')), sep = ';')
# x <- load_data('http://groupware.les.inf.puc-rio.br/static/WLE/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv')
```

```{r load_data, cache = TRUE}
load_data <- function(file_url, data_dir = 'data', cache = FALSE){
    # load the data model from the url
    file_path <- url(file_url)
    if(cache){
        if(!dir.exists(data_dir)) dir.create(data_dir)
        file_path <- file.path(data_dir, basename(file_url))
        if(!file.exists(file_path)) download.file(file_url, file_path, quiet = TRUE)
        if(!file.exists(file_path)) stop('Unable to download file ', file_url)
    }
    # clean the dummy values like '', 'NA', '#DIV/0!'
    read.csv(file_path, na.strings = c('', 'NA', '#DIV/0!'))
    # NOTE : if new / transformed columns are needed
    # transform(
    #     read.csv(file_path, na.strings = c('', 'NA', '#DIV/0!')),
    #     raw_timestamp = as.POSIXct(as.integer(as.numeric(as.character(raw_timestamp_part_1))), origin = '1970-01-01', tz = 'GMT'),
    #     cvtd_timestamp = strptime(cvtd_timestamp, format = '%d/%m/%Y %H:%M')
    # )
}

pml_training <- load_data('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
dim(pml_training)
```

A quick look at the data highlight few points.

```{r exploratory}
set.seed(9999)

# hist(as.numeric(pml_training$classe), main = 'Histogram of classes', xlab = 'classe')
# aggregate(training$classe, list(classe = training$classe), length)
table(pml_training$classe)
```
The testing data doesn't contains a `classe` variable to be compared with.
The `classe` elements seems pretty well balanced across the training data set although class `A` is over represented. Note that class `A` corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

## Many missing values
```{r na}
# sapply(pml_training[, sapply(pml_training, function(x) sum(is.na(x)) < nrow(pml_training) * 0.95)], function(x) sum(is.na(x)))
high_na <- colSums(is.na(pml_training)) / nrow(pml_training)
# high_na <- sapply(pml_training, function(x) sum(is.na(x)) / nrow(pml_training))
highest_na <- high_na[high_na >= 0.05]
```
An exploratory of the variables values shows that many variables contains a high ratio of missing values. `r round(length(highest_na) * 100 / length(high_na), 2)`% of the variables filtered on a $rate > 0.05$ contains at least `r round(min(highest_na) * 100, 2)`% of `NA`s values.

This rate is too high to impute data in the training data set with confidence.

## Build tidy datas set
```{r tidy_data}
names(pml_training[, grep('num_window|belt|forearm|arm|dumbbell|classe', names(pml_training), invert = TRUE)])

# library(caret)
tidy_data <- function(raw_data, treshold = 0.95){
    # NOTE : select predictors minus variables where the value is nearly the same
    tidy_predictor <- raw_data[, setdiff(grep('num_window|belt|forearm|arm|dumbbell|classe', names(raw_data)), nearZeroVar(raw_data))]
    # NOTE : keep column if NA ratio < 0.95
    ceil <- nrow(tidy_predictor) * treshold
    # tidy_predictor[, sapply(tidy_predictor, function(x) sum(is.na(x)) < ceil)]
    tidy_predictor[, colSums(is.na(tidy_predictor)) < ceil]
}

tidy_training  <- tidy_data(pml_training)
classe_col <- which(colnames(tidy_training) == 'classe')
dim(tidy_training)
```
The predictors have been selected because of the naming related to the sensor types, plus the num_windows which after some digging help to group the records. 

As this is a classification prediction problem, I decided to try 2 appoaches :
1. Random Forest
2. Regression Trees

The training dataset have been splited in half, one part for each training type. Then each subset is separated between a training data set and a validation dataset.

```{r building_data}
inBuild <- createFolds(y = tidy_training$classe, k = 2)
training <- tidy_training[inBuild[[1]], ]
validation <- tidy_training[-inBuild[[2]], ]

dim(training)
dim(validation)
```

# Analysis
## Random Forest
I'll first start with the random forest as it usually provides more accurate predictions with a cost of more computation power.

Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.
```{r random_forest, cache = TRUE}
# library(ggplot2)
# library(randomForest)
# library(beepr)
modRf <- train(classe ~ ., data = training, method = 'rf', prox = TRUE, trControl = trainControl(method = 'cv', number = 10, allowParallel = TRUE, verboseIter = FALSE))
# No k-mean cross validation, Bootstrap resampling method 1 resampling iteration
# Optimal setting, but overkill in the present case
# modRf <- train(classe ~ ., data = training, method = 'rf', prox = TRUE, trControl = trainControl(method = 'boot', number = 1, allowParallel = TRUE))
# beep()
modRf
# modRf$resample
varImp(modRf)
confusionMatrix.train(modRf)
predRf <- predict(modRf, validation)
cmRf <- confusionMatrix(predRf, validation$classe)
cmRf
# Print out model summary
# print(modRf$finalModel, digits=3)
```
As expected the Random forest prediction accuracy (`r round(cmRf$overall[[1]], 5)`) is very high, which makes the outcome quite reliable.

## Decision tree
Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves).

The second prediction model used to predict the classes is a Regression tree. It worth noting that the default setting for the `caret` package is quite weak with this data set. The `rpart` function default parameters provide a better prediction. This prediction can be reach by tweaking the caret params though.

```{r rpart, cache = TRUE}
# library(rpart)
# NOTE : this result can also be obtain using
# modRpart <- rpart(classe ~ ., data = training, method = 'class')
# predRpart <- predict(modRpart, validation, type = 'class')
# Here to get more accute outcome trainControl need to be disabled and cp set to 0.0001
# It's a case where caret default optimisation are actually performing worst than rpart basic setup
modRpart <- train(classe ~ ., method = 'rpart', data = training, trControl = trainControl(method = 'cv', number = 10, allowParallel = TRUE, verboseIter = FALSE), tuneGrid = data.frame(cp = 0.0001))
modRpart
confusionMatrix.train(modRpart)
predRpart <- predict(modRpart, validation)
cmRpart <- confusionMatrix(predRpart, validation$classe)
cmRpart
```
The Decision tree prediction accuracy (`r round(cmRpart$overall[[1]], 5)`) is weaker than the Random forest prediction, even if it performs above the average.

```{r fancyrpart, echo=FALSE}
# library(rattle)
fancyRpartPlot(modRpart$finalModel)
```

# Test cases predictions
The trained machine learning algorithm generated above are now used to predict 20 test cases.

```{r test_cases}
pml_testing <- load_data('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
dim(pml_testing)
# variables in training not present in testing
setdiff(names(pml_training), names(pml_testing))
# variables in testing not present in training
setdiff(names(pml_testing), names(pml_training))
testing <- tidy_data(pml_testing)
testing <- testing[, intersect(names(testing), names(tidy_training))]
dim(testing)
testingRf <- predict(modRf, testing)
testingRf
testingRpartPred <- predict(modRpart$finalModel, testing)
testingRpart <- factor(colnames(testingRpartPred)[apply(testingRpartPred, 1, function(x) which.max(x))])
testingRpart
table(testingRpart, testingRf)
```
```{r data_sample, echo = FALSE}
# library(pgmm)
# stargazer::stargazer(table(testingRpart, testingRf), title = 'Testing Comparison', type = 'html')
```
There is a `r sum(testingRpart == testingRf) * 100 / nrow(testing)`% of prediction matching between the 2 models.

# Conclusion
The Random Forest prediction model is a clear winner to predict the excercice `classe` out of the available predictors.

The Random forest prediction is very sensitive to the settings parameters used to train the algorithm. It produce a decent prediction though, but not good enough to par with the Random forest predictions.

```{r echo=FALSE}
# stopCluster(cl)
stopImplicitCluster()
```
